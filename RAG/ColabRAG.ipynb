{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d7ffedef",
      "metadata": {
        "id": "d7ffedef"
      },
      "source": [
        "### Installation of dependency not built-in Google Colab\n",
        "\n",
        "Byaldi==0.0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2cb26c",
      "metadata": {
        "id": "7c2cb26c",
        "vscode": {
          "languageId": "bat"
        }
      },
      "outputs": [],
      "source": [
        "!pip install Byaldi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e37fbf",
      "metadata": {
        "id": "00e37fbf",
        "vscode": {
          "languageId": "bat"
        }
      },
      "outputs": [],
      "source": [
        "!apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cPhFFyTFMhc3",
      "metadata": {
        "id": "cPhFFyTFMhc3"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import base64\n",
        "\n",
        "# Initialiser le client OpenAI\n",
        "client = OpenAI(\n",
        "    api_key=\"JpQAceF80Htu21tCbZx8eZGHwye2UP2V\",\n",
        "    base_url=\"https://llm.intellisphere.fr:9081/v1\"\n",
        ")\n",
        "\n",
        "model_name = client.models.list().data[0].id\n",
        "print(f\"[image_query] Mod√®le utilis√© : {model_name}\")\n",
        "\n",
        "def query_image_base64(img_base64: str, question: str) -> str:\n",
        "    try:\n",
        "        # Construire le message\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": question},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"}}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Faire la requ√™te\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=messages,\n",
        "            max_tokens=200\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Erreur lors de la requ√™te image+texte : {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cccfd31",
      "metadata": {
        "id": "2cccfd31"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from byaldi import RAGMultiModalModel\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "# D√©finir les chemins\n",
        "model_name = \"vidore/colqwen2-v0.1\"\n",
        "local_model_dir = Path(\"./models\") / model_name.replace(\"/\", \"_\")\n",
        "index_name = \"onisr_index\"\n",
        "\n",
        "pdf_path = Path(\"./content/pdf_folder/repport2024.pdf\")\n",
        "images_dir = Path(\"./content/images\")\n",
        "\n",
        "# Cr√©er les r√©pertoires si n√©cessaire\n",
        "local_model_dir.mkdir(parents=True, exist_ok=True)\n",
        "images_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# 1. Charger ou t√©l√©charger le mod√®le\n",
        "if local_model_dir.exists() and any(local_model_dir.iterdir()):\n",
        "    print(f\"Chargement du mod√®le local depuis {local_model_dir}\")\n",
        "    model = RAGMultiModalModel.from_pretrained(str(local_model_dir))\n",
        "else:\n",
        "    print(f\"T√©l√©chargement du mod√®le {model_name}\")\n",
        "    model = RAGMultiModalModel.from_pretrained(model_name)\n",
        "    print(dir(model))\n",
        "\n",
        "# 2. V√©rifier si l'index existe d√©j√†\n",
        "index_path = Path(f\"./{index_name}\")\n",
        "if index_path.exists():\n",
        "    print(f\"Chargement de l'index existant depuis {index_path}\")\n",
        "    model.load_index(index_name=index_name, index_path=str(index_path))\n",
        "else:\n",
        "    print(f\"Cr√©ation d'un nouvel index pour {pdf_path}\")\n",
        "    model.index(input_path=pdf_path,\n",
        "                index_name=index_name,\n",
        "                store_collection_with_index=True,\n",
        "                overwrite=True)\n",
        "\n",
        "# 3. Convertir le PDF en images une seule fois\n",
        "if not any(images_dir.iterdir()):\n",
        "    print(f\"Conversion du PDF en images et sauvegarde dans {images_dir}\")\n",
        "    images = convert_from_path(pdf_path)\n",
        "    for i, image in enumerate(images):\n",
        "        image.save(images_dir / f\"page_{i+1}.png\", \"PNG\")\n",
        "else:\n",
        "    print(f\"Images d√©j√† pr√©sentes dans {images_dir}, chargement direct\")\n",
        "\n",
        "# Requ√™te de recherche\n",
        "query = \"Quelle est le sexe qui fait le plus d'accident ?\"\n",
        "results = model.search(query, k=1)\n",
        "\n",
        "print(f\"R√©sultats de la recherche pour '{query}':\")\n",
        "for result in results:\n",
        "    print(f\"Doc ID: {result.doc_id}, Page: {result.page_num}, Score: {result.score}\")\n",
        "\n",
        "# Charger l'image de la page retourn√©e\n",
        "page_num = model.search(query, k=1)[0].page_num\n",
        "image_path = images_dir / f\"page_{page_num}.png\"\n",
        "\n",
        "# Lire l'image en base64\n",
        "with open(image_path, \"rb\") as image_file:\n",
        "    import base64\n",
        "    returned_page = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "response = query_image_base64(returned_page, query)\n",
        "print(\"R√©ponse image + texte :\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VBYN8dP0OXWt",
      "metadata": {
        "id": "VBYN8dP0OXWt"
      },
      "source": [
        "### Push request sur GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rz0bTWQZOZ-J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "rz0bTWQZOZ-J",
        "outputId": "5e3c7559-5d0d-46f6-a050-8818424480cb"
      },
      "outputs": [],
      "source": [
        "# üìå Informations :\n",
        "GITHUB_TOKEN = \"###\"  # üîê GitHub Token\n",
        "GITHUB_USERNAME = \"SullyvanCOULON\"             # GitHub Username\n",
        "REPO_NAME = \"ProjectE4\"                        # Nom du repo\n",
        "BRANCH_NAME = \"main\"                           # Branche cible\n",
        "TARGET_PATH = \"RAG/ColabRAG.ipynb\"             # Chemin souhait√© dans le repo\n",
        "COMMIT_MESSAGE = \"Cr√©√© √† l'aide de Colab ‚Äî Modification pour int√©grer 'AccesLLM', erreur Request timed out\"\n",
        "\n",
        "# üìÅ Nom du fichier local (le notebook actuel)\n",
        "import os\n",
        "local_nb = \"ColabRAG.ipynb\"\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "# üîÑ Sauvegarde le notebook actuel localement\n",
        "display(Javascript('IPython.notebook.save_checkpoint();'))\n",
        "\n",
        "# ‚è≥ Attente que la sauvegarde soit faite\n",
        "import time\n",
        "time.sleep(3)\n",
        "\n",
        "# üì• Installer Git si besoin\n",
        "!apt-get install git -y\n",
        "\n",
        "# üõ†Ô∏è Configurer Git\n",
        "!git config --global user.email \"sullyvancoulon@icloud.com\"\n",
        "!git config --global user.name \"{GITHUB_USERNAME}\"\n",
        "\n",
        "# üì¶ Cloner le d√©p√¥t avec authentification\n",
        "repo_url = f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
        "!rm -rf {REPO_NAME}\n",
        "!git clone --branch {BRANCH_NAME} {repo_url}\n",
        "\n",
        "# üì§ Copier le notebook dans le dossier cible\n",
        "import shutil\n",
        "shutil.copy(local_nb, f\"{REPO_NAME}/{TARGET_PATH}\")\n",
        "\n",
        "# üíæ Commit et push\n",
        "%cd {REPO_NAME}\n",
        "!git add {TARGET_PATH}\n",
        "!git commit -m \"{COMMIT_MESSAGE}\"\n",
        "!git push origin {BRANCH_NAME}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "projE4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
